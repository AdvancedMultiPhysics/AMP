#ifndef RAD_DIF_MONOLITHIC_PREC
#define RAD_DIF_MONOLITHIC_PREC


#include "AMP/IO/PIO.h"
#include "AMP/IO/AsciiWriter.h"
#include "AMP/utils/AMPManager.h"

#include "AMP/vectors/CommunicationList.h"
#include "AMP/matrices/petsc/NativePetscMatrix.h"
#include "AMP/vectors/VectorBuilder.h"
#include "AMP/vectors/Vector.h"
#include "AMP/vectors/MultiVector.h"
#include "AMP/vectors/MultiVariable.h"
#include "AMP/vectors/data/VectorData.h"
#include "AMP/vectors/data/VectorDataNull.h"
#include "AMP/vectors/operations/default/VectorOperationsDefault.h"
#include "AMP/vectors/VectorBuilder.h"

#include "AMP/discretization/boxMeshDOFManager.h"
#include "AMP/discretization/MultiDOF_Manager.h"
#include "AMP/mesh/Mesh.h"
#include "AMP/mesh/MeshID.h"
#include "AMP/mesh/MeshParameters.h"
#include "AMP/mesh/MeshElement.h"
#include "AMP/mesh/structured/BoxMesh.h"

#include "AMP/matrices/CSRMatrix.h"
#include "AMP/matrices/MatrixBuilder.h"

#include "AMP/operators/Operator.h"
#include "AMP/operators/OperatorParameters.h"
#include "AMP/operators/LinearOperator.h"
#include "AMP/operators/petsc/PetscMatrixShellOperator.h"
#include "AMP/operators/OperatorFactory.h"

#include "AMP/solvers/SolverFactory.h"
#include "AMP/solvers/SolverStrategy.h"
#include "AMP/solvers/testHelpers/SolverTestParameters.h"
#include "AMP/solvers/SolverStrategyParameters.h"
#include "AMP/solvers/SolverStrategy.h"
#include "AMP/solvers/SolverFactory.h"
#include "AMP/solvers/petsc/PetscSNESSolver.h"


#include "RDUtils.h"
#include "RadiationDiffusionFDBEWrappers.h"
#include "RadiationDiffusionFDDiscretization.h"

#include <iostream>
#include <iomanip>

//class BERadDifOpPJac;

// todo: what's the correct name space here.
namespace AMP::Operator {

#if 0

/*
6.8.6 AMG for systems of PDEs
If the users wants to solve systems of PDEs and can provide information on which variables belong to which function, BoomerAMG’s systems AMG version can also be used. Functions that enable the user to access the systems AMG version are:
        a. HYPRE_BoomerAMGSetNumFunctions, 
        b. HYPRE_BoomerAMGSetDofFunc   
        c. HYPRE_BoomerAMGSetNodal.

    a. HYPRE_Int HYPRE_BoomerAMGSetNumFunctions(HYPRE_Solver solver, HYPRE_Int num_functions)
    (Optional) Sets the size of the system of PDEs, if using the systems version.
    The default is 1, i.e. a scalar system.
    ---this option is in AMP as "num_functions"
    ---I will set this to "2"

    b. HYPRE_Int HYPRE_BoomerAMGSetDofFunc(HYPRE_Solver solver, HYPRE_Int *dof_func)
    (Optional) Sets the mapping that assigns the function to each variable, if using the systems version. If no assignment is made and the number of functions is k > 1, the mapping generated is (0,1,. . . ,k-1,0,1,. . . ,k-1,. . . ).
    ---this is NOT available in AMP
    ---Bit confused: Did some checking chat GPT (it went around in a circle). This function maps from a row in the matrix to the index (0,1,...,k-1) of the variable that owns the row. If you build the matrix in nodal ordering then it has the default structure provided by this function.

    c. HYPRE_Int HYPRE_BoomerAMGSetNodal(HYPRE_Solver solver, HYPRE_Int nodal)
    (Optional) Sets whether to use the nodal systems coarsening.
    Should be used for linear systems generated from systems of PDEs. The default is 0 (unknown-based coarsening, only coarsens within same function). For the remaining options a nodal matrix is generated by applying a norm to the nodal blocks and applying the coarsening algorithm to this matrix.
    • 1 : Frobenius norm
    • 2 : sum of absolute values of elements in each block
    • 3 : largest element in each block (not absolute value)
    • 4 : row-sum norm
    • 6 : sum of all values in each block
    ---this option is in AMP as "nodal"


Smoothing:
==========
"To use the more complicated smoothers, e.g. block, Schwarz, ILU smoothers, it is necessary to use HYPRE_BoomerAMGSetSmoothType and HYPRE_BoomerAMGSetSmoothNumLevels"
---these options are in AMP with:
    "smooth_type" and "smooth_number_levels"

    * HYPRE_Int HYPRE_BoomerAMGSetSmoothNumLevels(HYPRE_Solver solver, HYPRE_Int
    smooth_num_levels)
    (Optional) Sets the number of levels for more complex smoothers.
    The smoothers, as defined by HYPRE_BoomerAMGSetSmoothType, will be used on level 0 (the finest
    level) through level smooth_num_levels-1. The default is 0, i.e. no complex smoothers are used

    * HYPRE_Int HYPRE_BoomerAMGSetSmoothType(HYPRE_Solver solver, HYPRE_Int smooth_type)
        (Optional) Enables the use of more complex smoothers.
    --this option is in AMP with "smooth_type"
    
    The following options exist for smooth_type
    • 6 : Schwarz (routines needed to set: 
            a. HYPRE_BoomerAMGSetDomainType,
            b. HYPRE_BoomerAMGSetOverlap, 
            c. HYPRE_BoomerAMGSetVariant,
            d. HYPRE_BoomerAMGSetSchwarzRlxWeight
        )

        a. HYPRE_Int HYPRE_BoomerAMGSetDomainType(HYPRE_Solver solver, HYPRE_Int domain_type)
        Defines the type of domain used for the Schwarz method.
        The following options exist for domain_type:
        • 0 : each point is a domain
        • 1 : each node is a domain (only of interest in “systems” AMG)
        • 2 : each domain is generated by agglomeration (default)
        --this option is in AMP with "schwarz_domain_type"

        b. HYPRE_Int HYPRE_BoomerAMGSetOverlap(HYPRE_Solver solver, HYPRE_Int overlap)
        (Optional) Defines the overlap for the Schwarz method.
        The following options exist for overlap:
        • 0 : no overlap
        • 1 : minimal overlap (default)
        • 2 : overlap generated by including all neighbors of domain boundaries
        --this option is NOT in AMP

        c. HYPRE_Int HYPRE_BoomerAMGSetVariant(HYPRE_Solver solver, HYPRE_Int variant)
        (Optional) Defines which variant of the Schwarz method is used.
        The following options exist for variant:
        • 0 : hybrid multiplicative Schwarz method (no overlap across processor boundaries)
        • 1 : hybrid additive Schwarz method (no overlap across processor boundaries)
        • 2 : additive Schwarz method
        • 3 : hybrid multiplicative Schwarz method (with overlap across processor boundaries)
        The default is 0.
        --this option is in AMP with "schwarz_variant"

        d. HYPRE_Int HYPRE_BoomerAMGSetSchwarzRlxWeight(HYPRE_Solver solver, HYPRE_Real
        schwarz_rlx_weight)
        (Optional) Defines a smoothing parameter for the additive Schwarz method
        --this option is in AMP with "schwarz_weight"


*/



class BERadDifOpPJacMonolithic : public AMP::Solver::SolverStrategy {

public:

    // Keep a pointer to this to save having to down cast more than once 
    std::shared_ptr<BERadDifOpPJac> d_BERadDifOpPJac = nullptr;

    // Solver
    std::shared_ptr<AMP::Solver::SolverStrategy> d_solver = nullptr;

    // The base class has the following data:
    // d_iMaxIterations       = "max_iterations"
    // d_iDebugPrintInfoLevel = "print_info_level"
    // d_bUseZeroInitialGuess = "zero_initial_guess"
    // d_dAbsoluteTolerance   = "absolute_tolerance"
    // d_dRelativeTolerance   = "relative_tolerance"
    // d_bComputeResidual     = "compute_residual"

    BERadDifOpPJacMonolithic( std::shared_ptr<AMP::Solver::SolverStrategyParameters> params )
    : SolverStrategy( params ) {

        if ( d_iDebugPrintInfoLevel > 1 )
            AMP::pout << "BERadDifOpPJacMonolithic::BERadDifOpPJacMonolithic() " << std::endl;
    };  

    // Used by SolverFactory to create a BERadDifOpPJacMonolithic
    static std::unique_ptr<AMP::Solver::SolverStrategy> create( std::shared_ptr<AMP::Solver::SolverStrategyParameters> params ) {  
        return std::make_unique<BERadDifOpPJacMonolithic>( params ); };

    // Implementation of pure virtual function
    std::string type() const { return "BERadDifOpPJacMonolithic"; };

    // Apply preconditioner 
    // Incoming vectors are in variable ordering
    void apply(std::shared_ptr<const AMP::LinearAlgebra::Vector> bET_, std::shared_ptr< AMP::LinearAlgebra::Vector> ET_) override {

        if ( d_iDebugPrintInfoLevel > 2 )
            AMP::pout << "BERadDifOpPJacMonolithic::apply() " << std::endl;


        // I don't think it makes sense to use a non-zero initial guess, does it?
        AMP_INSIST( d_bUseZeroInitialGuess, "Zero initial guess is hard coded!" );

        // Current implementation only supports a fixed number of iterations, ignores tolerances
        AMP_INSIST( d_dAbsoluteTolerance == 0.0 && d_dRelativeTolerance == 0.0, "Non-zero tolerances not implemented; only fixed number of iterations" );

        AMP_INSIST( this->getOperator(), "Apply requires an operator to be registered" );
        // Sometimes the d_pOperator variable is referenced directly rather than calling registerOperator... This call ensures our overridden registerOperator gets called
        registerOperator( this->getOperator() );


        // Convert variable-ordered vectors into nodal ordering
        // Create vectors for nodal ordering
        auto bET_ndl = d_BERadDifOpPJac->createNodalInputVector();
        auto ET_ndl  = d_BERadDifOpPJac->createNodalInputVector();
        d_BERadDifOpPJac->createNodalOrderedCopy( bET_, bET_ndl );
        d_BERadDifOpPJac->createNodalOrderedCopy( ET_, ET_ndl );
        ET_ndl->makeConsistent( AMP::LinearAlgebra::ScatterType::CONSISTENT_SET );
        bET_ndl->makeConsistent( AMP::LinearAlgebra::ScatterType::CONSISTENT_SET );

        // Create a residual and correction vectors 
        auto rET_ndl = d_BERadDifOpPJac->createNodalInputVector();
        auto dET_ndl = d_BERadDifOpPJac->createNodalInputVector();


        // Assemble solver
        setSolver();
        
        // --- Iterate ---
        // Set initial iterate to zero
        ET_ndl->zero( );
        for ( auto iter = 0; iter < d_iMaxIterations; iter++ ) {

            // Compute residual
            if ( iter == 0 ) {
                rET_ndl->copyVector( bET_ndl ); // Zero initial iterate means r0 = b - A*x0 = b
            } else {
                d_BERadDifOpPJac->residualNodal( bET_ndl, ET_ndl, rET_ndl );
            }
            if ( d_iDebugPrintInfoLevel > 1 ) {
                AMP::pout << "BERadDifOpPJacMonolithic::apply(): iteration " << iter << ":" << std::endl;
                auto rnorms = getDiscreteNorms( 1.0, rET_ndl );
                AMP::pout << "||r||=(" << rnorms[0] << "," << rnorms[1] << "," << rnorms[2] << ")" << std::endl;
            }

            // Solve M * dET = rET, for precoditioner M
            d_solver->apply( rET_ndl, dET_ndl );

            // Update solution: ET <- ET + rET
            ET_ndl->axpby( 1.0, 1.0, *dET_ndl ); // ET <- 1.0*ET + 1.0*dET 
            ET_ndl->makeConsistent( AMP::LinearAlgebra::ScatterType::CONSISTENT_SET );
        }

        // Display final residual; note that this residual calculation is unnecessary because the final iterate is calculated already, hence the flag 
        if ( d_bComputeResidual ) {
            AMP::pout << "BERadDifOpPJacMonolithic::apply(): final residual" << ":" << std::endl;
            d_BERadDifOpPJac->residualNodal( bET_ndl, ET_ndl, rET_ndl );
            auto rnorms = getDiscreteNorms( 1.0, rET_ndl );
            AMP::pout << "||r||=(" << rnorms[0] << "," << rnorms[1] << "," << rnorms[2] << ")" << std::endl;
        }

        // Copy output vector back to variable ordering
        d_BERadDifOpPJac->createVariableOrderedCopy( ET_ndl, ET_ );
    }

    // Create solvers for diffusion blocks if they don't exist already
    void setSolver( ) {

        if ( d_solver )
            return;

        // Wrap matrix as LinearOperators 
        auto db    = AMP::Database::create( "name", "Operator", "print_info_level", 0 );
        auto params = std::make_shared<AMP::Operator::OperatorParameters>( std::move(db) );
        auto JacNodal = std::make_shared<AMP::Operator::LinearOperator>( params );
        JacNodal->setMatrix( d_BERadDifOpPJac->d_JNodal );
        AMP_INSIST( JacNodal->getMatrix(), "Jac Matrix is null" );

        // Create solver parameters
        auto comm        = d_BERadDifOpPJac->getMesh()->getComm(); 
        auto solver_db   = d_db->getDatabase( "MonolithicSolver" );
        auto solverParams = std::make_shared<AMP::Solver::SolverStrategyParameters>( solver_db );
        solverParams->d_pOperator = JacNodal;
        solverParams->d_comm      = comm;

        // Create solvers from Factories
        d_solver = AMP::Solver::SolverFactory::create( solverParams );
        // Ensure zero initial guess is used
        d_solver->setZeroInitialGuess( true );
    }


    void reset( std::shared_ptr<AMP::Solver::SolverStrategyParameters> params ) override {
        
        if ( d_iDebugPrintInfoLevel > 1 )
            AMP::pout << "BERadDifOpPJacMonolithic::reset() " << std::endl;

        d_solver = nullptr;
    }

    void registerOperator( std::shared_ptr<AMP::Operator::Operator> op ) override {
        if ( d_iDebugPrintInfoLevel > 1 )
            AMP::pout << "BERadDifOpPJacMonolithic::registerOperator() " << std::endl;

        AMP_INSIST( op, "A null operator cannot be registered" );

        d_pOperator = op;
        auto myBEOp = std::dynamic_pointer_cast<BERadDifOpPJac>( op );
        AMP_INSIST( myBEOp, "Operator must be of BERadDifOpPJac type" );
        d_BERadDifOpPJac = myBEOp;
    }
};




/*
Maybe it's just easiest to create a map from a nodal index to a variable index and vice versa

Say we have 2*n values on process (n for T) and (n for E), indices a,a+1, ..., a+2*n-1, where a is an even integer

Then variable ordering is
[a,a+1,a+2,a+3,   ...,a+(n-1)],  [a+n,a+1+n,...a+n-1+(n-1),a+n+(n-1)]
And the nodal ordering is
 a,a+n,a+1,a+1+n, ...,                      a+(n-1),a+n+(n-1) 


// This is only going to work if I know a... or for local DOFs where a=0
but that's an issue because sometimes I have connections to non-local DOFs...

For a given index, I need to know the first index on that process and how many dofs are on that process...


# Consider the following variably-ordered indices mapped to nodally-ordered indices
# Variable ordering
P0: [0 1 2 3 4] [5 6 7 8 9 10]
P1: [11 12 13 14] [16 17 18 19]

# Nodal ordering
P0: [0 5 1 6 2 7 3 8 4 9 5 10]
P1: [11 16 12 17 13 18 14 19]

So say I'm given DOFs [0 16] then I have an issue because they're on different proccesses with different starting points and different numbers of DOFs on each...

Just assume serial for the moment.... 
*/

// 0->0, 1->n, 2->1, 3->n+1 ...
// So even nodal indices get divided by 2
// and odd nodal indices get subtract 1, divide by 2 and add n  
inline void nodalOrderingToVariableOrdering( size_t n, const std::vector<size_t> &ndl, std::vector<size_t> &var ) {
    var.resize( ndl.size() );
    for ( auto i = 0; i < ndl.size(); i++ ) {
        auto dof = ndl[i];
        if ( dof % 2 == 0 ) {
            var[i] = dof/2;
        } else {
            var[i] = (dof-1)/2 + n;
        }
    }
}

// 0->0, 1->2, 2->4, ..., n->1, 1+n->3, 2+n->5
// So variable indices  <n get doubled
// and variable indices >= subtract n, double and add 1
inline void variableOrderingToNodalOrdering( size_t n, const std::vector<size_t> &var, std::vector<size_t> &ndl ) {
    ndl.resize( var.size() );
    for ( auto i = 0; i < var.size(); i++ ) {
        auto dof = var[i];
        if ( dof < n ) {
            ndl[i] = dof * 2;
        } else {
            ndl[i] = (dof-n) * 2 + 1;
        }
    }
}


#endif

} // namespace AMP::Operator


#endif